{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Summarize",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wwyy6qvnb1c",
        "colab_type": "text"
      },
      "source": [
        "추상적 요약을 이용할 때는 레이블링 된 데이터가 있어야함.\n",
        "\n",
        "추출적 요약으로 얻은 데이터를 라벨로하여 fine-tuning을 시켜보는 건?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsv40DGDnnk9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# rank.py\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "def pagerank(x, df=0.85, max_iter=30, bias=None):\n",
        "\n",
        "    assert 0 < df < 1\n",
        "\n",
        "    # initialize\n",
        "    A = normalize(x, axis=0, norm='l1')\n",
        "    R = np.ones(A.shape[0]).reshape(-1,1)\n",
        "\n",
        "    # check bias\n",
        "    if bias is None:\n",
        "        bias = (1 - df) * np.ones(A.shape[0]).reshape(-1,1)\n",
        "    else:\n",
        "        bias = bias.reshape(-1,1)\n",
        "        bias = A.shape[0] * bias / bias.sum()\n",
        "        assert bias.shape[0] == A.shape[0]\n",
        "        bias = (1 - df) * bias\n",
        "\n",
        "    # iteration\n",
        "    for _ in range(max_iter):\n",
        "        R = df * (A * R) + bias\n",
        "\n",
        "    return R\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bA3547SWn4rY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sentence.py\n",
        "from collections import Counter\n",
        "import math\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.metrics import pairwise_distances\n",
        "\n",
        "def sent_graph(sents, tokenize=None, min_count=2, min_sim=0.3, similarity=None, vocab_to_idx=None, verbose=False):\n",
        "    if vocab_to_idx is None:\n",
        "        idx_to_vocab, vocab_to_idx = scan_vocabulary(sents, tokenize, min_count)\n",
        "    else:\n",
        "        idx_to_vocab = [vocab for vocab, _ in sorted(vocab_to_idx.items(), key=lambda x:x[1])]\n",
        "\n",
        "    x = vectorize_sents(sents, tokenize, vocab_to_idx)\n",
        "    if similarity == 'cosine':\n",
        "        x = numpy_cosine_similarity_matrix(x, min_sim, verbose, batch_size=1000)\n",
        "    else:\n",
        "        x = numpy_textrank_similarity_matrix(x, min_sim, verbose, batch_size=1000)\n",
        "    return x\n",
        "\n",
        "def vectorize_sents(sents, tokenize, vocab_to_idx):\n",
        "    rows, cols, data = [], [], []\n",
        "    for i, sent in enumerate(sents):\n",
        "        counter = Counter(tokenize(sent))\n",
        "        for token, count in counter.items():\n",
        "            j = vocab_to_idx.get(token, -1)\n",
        "            if j == -1:\n",
        "                continue\n",
        "            rows.append(i)\n",
        "            cols.append(j)\n",
        "            data.append(count)\n",
        "    n_rows = len(sents)\n",
        "    n_cols = len(vocab_to_idx)\n",
        "    return csr_matrix((data, (rows, cols)), shape=(n_rows, n_cols))\n",
        "\n",
        "def numpy_cosine_similarity_matrix(x, min_sim=0.3, verbose=True, batch_size=1000):\n",
        "    n_rows = x.shape[0]\n",
        "    mat = []\n",
        "    for bidx in range(math.ceil(n_rows / batch_size)):\n",
        "        b = int(bidx * batch_size)\n",
        "        e = min(n_rows, int((bidx+1) * batch_size))\n",
        "        psim = 1 - pairwise_distances(x[b:e], x, metric='cosine')\n",
        "        rows, cols = np.where(psim >= min_sim)\n",
        "        data = psim[rows, cols]\n",
        "        mat.append(csr_matrix((data, (rows, cols)), shape=(e-b, n_rows)))\n",
        "        if verbose:\n",
        "            print('\\rcalculating cosine sentence similarity {} / {}'.format(b, n_rows), end='')\n",
        "    mat = sp.sparse.vstack(mat)\n",
        "    if verbose:\n",
        "        print('\\rcalculating cosine sentence similarity was done with {} sents'.format(n_rows))\n",
        "    return mat\n",
        "\n",
        "def numpy_textrank_similarity_matrix(x, min_sim=0.3, verbose=True, min_length=1, batch_size=1000):\n",
        "    n_rows, n_cols = x.shape\n",
        "\n",
        "    # Boolean matrix\n",
        "    rows, cols = x.nonzero()\n",
        "    data = np.ones(rows.shape[0])\n",
        "    z = csr_matrix((data, (rows, cols)), shape=(n_rows, n_cols))\n",
        "\n",
        "    # Inverse sentence length\n",
        "    size = np.asarray(x.sum(axis=1)).reshape(-1)\n",
        "    size[np.where(size <= min_length)] = 10000\n",
        "    size = np.log(size)\n",
        "\n",
        "    mat = []\n",
        "    for bidx in range(math.ceil(n_rows / batch_size)):\n",
        "\n",
        "        # slicing\n",
        "        b = int(bidx * batch_size)\n",
        "        e = min(n_rows, int((bidx+1) * batch_size))\n",
        "\n",
        "        # dot product\n",
        "        inner = z[b:e,:] * z.transpose()\n",
        "\n",
        "        # sentence len[i,j] = size[i] + size[j]\n",
        "        norm = size[b:e].reshape(-1,1) + size.reshape(1,-1)\n",
        "        norm = norm ** (-1)\n",
        "        norm[np.where(norm == np.inf)] = 0\n",
        "\n",
        "        # normalize\n",
        "        sim = inner.multiply(norm).tocsr()\n",
        "        rows, cols = (sim >= min_sim).nonzero()\n",
        "        data = np.asarray(sim[rows, cols]).reshape(-1)\n",
        "\n",
        "        # append\n",
        "        mat.append(csr_matrix((data, (rows, cols)), shape=(e-b, n_rows)))\n",
        "\n",
        "        if verbose:\n",
        "            print('\\rcalculating textrank sentence similarity {} / {}'.format(b, n_rows), end='')\n",
        "\n",
        "    mat = sp.sparse.vstack(mat)\n",
        "    if verbose:\n",
        "        print('\\rcalculating textrank sentence similarity was done with {} sents'.format(n_rows))\n",
        "\n",
        "    return mat\n",
        "\n",
        "def graph_with_python_sim(tokens, verbose, similarity, min_sim):\n",
        "    if similarity == 'cosine':\n",
        "        similarity = cosine_sent_sim\n",
        "    elif callable(similarity):\n",
        "        similarity = similarity\n",
        "    else:\n",
        "        similarity = textrank_sent_sim\n",
        "\n",
        "    rows, cols, data = [], [], []\n",
        "    n_sents = len(tokens)\n",
        "    for i, tokens_i in enumerate(tokens):\n",
        "        if verbose and i % 1000 == 0:\n",
        "            print('\\rconstructing sentence graph {} / {} ...'.format(i, n_sents), end='')\n",
        "        for j, tokens_j in enumerate(tokens):\n",
        "            if i >= j:\n",
        "                continue\n",
        "            sim = similarity(tokens_i, tokens_j)\n",
        "            if sim < min_sim:\n",
        "                continue\n",
        "            rows.append(i)\n",
        "            cols.append(j)\n",
        "            data.append(sim)\n",
        "    if verbose:\n",
        "        print('\\rconstructing sentence graph was constructed from {} sents'.format(n_sents))\n",
        "    return csr_matrix((data, (rows, cols)), shape=(n_sents, n_sents))\n",
        "\n",
        "def textrank_sent_sim(s1, s2):\n",
        "    n1 = len(s1)\n",
        "    n2 = len(s2)\n",
        "    if (n1 <= 1) or (n2 <= 1):\n",
        "        return 0\n",
        "    common = len(set(s1).intersection(set(s2)))\n",
        "    base = math.log(n1) + math.log(n2)\n",
        "    return common / base\n",
        "\n",
        "def cosine_sent_sim(s1, s2):\n",
        "    if (not s1) or (not s2):\n",
        "        return 0\n",
        "\n",
        "    s1 = Counter(s1)\n",
        "    s2 = Counter(s2)\n",
        "    norm1 = math.sqrt(sum(v ** 2 for v in s1.values()))\n",
        "    norm2 = math.sqrt(sum(v ** 2 for v in s2.values()))\n",
        "    prod = 0\n",
        "    for k, v in s1.items():\n",
        "        prod += v * s2.get(k, 0)\n",
        "    return prod / (norm1 * norm2)\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqjgA6W_n6UI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# summarizer.py\n",
        "import numpy as np\n",
        "\n",
        "class KeywordSummarizer:\n",
        "    def __init__(self, sents=None, tokenize=None, min_count=2,\n",
        "        window=-1, min_cooccurrence=2, vocab_to_idx=None,\n",
        "        df=0.85, max_iter=30, verbose=False):\n",
        "\n",
        "        self.tokenize = tokenize\n",
        "        self.min_count = min_count\n",
        "        self.window = window\n",
        "        self.min_cooccurrence = min_cooccurrence\n",
        "        self.vocab_to_idx = vocab_to_idx\n",
        "        self.df = df\n",
        "        self.max_iter = max_iter\n",
        "        self.verbose = verbose\n",
        "\n",
        "        if sents is not None:\n",
        "            self.train_textrank(sents)\n",
        "\n",
        "    def train_textrank(self, sents, bias=None):\n",
        "\n",
        "        g, self.idx_to_vocab = word_graph(sents,\n",
        "            self.tokenize, self.min_count,self.window,\n",
        "            self.min_cooccurrence, self.vocab_to_idx, self.verbose)\n",
        "        self.R = pagerank(g, self.df, self.max_iter, bias).reshape(-1)\n",
        "        if self.verbose:\n",
        "            print('trained TextRank. n words = {}'.format(self.R.shape[0]))\n",
        "\n",
        "    def keywords(self, topk=30):\n",
        "\n",
        "        if not hasattr(self, 'R'):\n",
        "            raise RuntimeError('Train textrank first or use summarize function')\n",
        "        idxs = self.R.argsort()[-topk:]\n",
        "        keywords = [(self.idx_to_vocab[idx], self.R[idx]) for idx in reversed(idxs)]\n",
        "        return keywords\n",
        "\n",
        "    def summarize(self, sents, topk=30):\n",
        "\n",
        "        self.train_textrank(sents)\n",
        "        return self.keywords(topk)\n",
        "\n",
        "\n",
        "class KeysentenceSummarizer:\n",
        "\n",
        "    def __init__(self, sents=None, tokenize=None, min_count=2,\n",
        "        min_sim=0.3, similarity=None, vocab_to_idx=None,\n",
        "        df=0.85, max_iter=30, verbose=False):\n",
        "\n",
        "        self.tokenize = tokenize\n",
        "        self.min_count = min_count\n",
        "        self.min_sim = min_sim\n",
        "        self.similarity = similarity\n",
        "        self.vocab_to_idx = vocab_to_idx\n",
        "        self.df = df\n",
        "        self.max_iter = max_iter\n",
        "        self.verbose = verbose\n",
        "\n",
        "        if sents is not None:\n",
        "            self.train_textrank(sents)\n",
        "\n",
        "    def train_textrank(self, sents, bias=None):\n",
        "\n",
        "        g = sent_graph(sents, self.tokenize, self.min_count,\n",
        "            self.min_sim, self.similarity, self.vocab_to_idx, self.verbose)\n",
        "        self.R = pagerank(g, self.df, self.max_iter, bias).reshape(-1)\n",
        "        if self.verbose:\n",
        "            print('trained TextRank. n sentences = {}'.format(self.R.shape[0]))\n",
        "\n",
        "    def summarize(self, sents, topk=30, bias=None):\n",
        "\n",
        "        n_sents = len(sents)\n",
        "        if isinstance(bias, np.ndarray):\n",
        "            if bias.shape != (n_sents,):\n",
        "                raise ValueError('The shape of bias must be (n_sents,) but {}'.format(bias.shape))\n",
        "        elif bias is not None:\n",
        "            raise ValueError('The type of bias must be None or numpy.ndarray but the type is {}'.format(type(bias)))\n",
        "\n",
        "        self.train_textrank(sents, bias)\n",
        "        idxs = self.R.argsort()[-topk:]\n",
        "        keysents = [(idx, self.R[idx], sents[idx]) for idx in reversed(idxs)]\n",
        "        return keysents\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-d9L8CSsn7wT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# utils.py\n",
        "from collections import Counter\n",
        "from scipy.sparse import csr_matrix\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def scan_vocabulary(sents, tokenize=None, min_count=2):\n",
        "\n",
        "    counter = Counter(w for sent in sents for w in tokenize(sent))\n",
        "    counter = {w:c for w,c in counter.items() if c >= min_count}\n",
        "    idx_to_vocab = [w for w, _ in sorted(counter.items(), key=lambda x:-x[1])]\n",
        "    vocab_to_idx = {vocab:idx for idx, vocab in enumerate(idx_to_vocab)}\n",
        "    return idx_to_vocab, vocab_to_idx\n",
        "\n",
        "def tokenize_sents(sents, tokenize):\n",
        "\n",
        "    return [tokenize(sent) for sent in sents]\n",
        "\n",
        "def vectorize(tokens, vocab_to_idx):\n",
        "\n",
        "    rows, cols, data = [], [], []\n",
        "    for i, tokens_i in enumerate(tokens):\n",
        "        for t, c in Counter(tokens_i).items():\n",
        "            j = vocab_to_idx.get(t, -1)\n",
        "            if j == -1:\n",
        "                continue\n",
        "            rows.append(i)\n",
        "            cols.append(j)\n",
        "            data.append(c)\n",
        "    n_sents = len(tokens)\n",
        "    n_terms = len(vocab_to_idx)\n",
        "    x = csr_matrix((data, (rows, cols)), shape=(n_sents, n_terms))\n",
        "    return x\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFrpXdhDn8-C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# word.py\n",
        "from collections import defaultdict\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "def word_graph(sents, tokenize=None, min_count=2, window=2, min_cooccurrence=2, vocab_to_idx=None, verbose=False):\n",
        "\n",
        "    if vocab_to_idx is None:\n",
        "        idx_to_vocab, vocab_to_idx = scan_vocabulary(sents, tokenize, min_count)\n",
        "    else:\n",
        "        idx_to_vocab = [vocab for vocab, _ in sorted(vocab_to_idx.items(), key=lambda x:x[1])]\n",
        "\n",
        "    tokens = tokenize_sents(sents, tokenize)\n",
        "    g = cooccurrence(tokens, vocab_to_idx, window, min_cooccurrence, verbose)\n",
        "    return g, idx_to_vocab\n",
        "\n",
        "def cooccurrence(tokens, vocab_to_idx, window=2, min_cooccurrence=2, verbose=False):\n",
        "\n",
        "    counter = defaultdict(int)\n",
        "    for s, tokens_i in enumerate(tokens):\n",
        "        if verbose and s % 1000 == 0:\n",
        "            print('\\rword cooccurrence counting {}'.format(s), end='')\n",
        "        vocabs = [vocab_to_idx[w] for w in tokens_i if w in vocab_to_idx]\n",
        "        n = len(vocabs)\n",
        "        for i, v in enumerate(vocabs):\n",
        "            if window <= 0:\n",
        "                b, e = 0, n\n",
        "            else:\n",
        "                b = max(0, i - window)\n",
        "                e = min(i + window, n)\n",
        "            for j in range(b, e):\n",
        "                if i == j:\n",
        "                    continue\n",
        "                counter[(v, vocabs[j])] += 1\n",
        "                counter[(vocabs[j], v)] += 1\n",
        "    counter = {k:v for k,v in counter.items() if v >= min_cooccurrence}\n",
        "    n_vocabs = len(vocab_to_idx)\n",
        "    if verbose:\n",
        "        print('\\rword cooccurrence counting from {} sents was done'.format(s+1))\n",
        "    return dict_to_mat(counter, n_vocabs, n_vocabs)\n",
        "\n",
        "def dict_to_mat(d, n_rows, n_cols):\n",
        "\n",
        "    rows, cols, data = [], [], []\n",
        "    for (i, j), v in d.items():\n",
        "        rows.append(i)\n",
        "        cols.append(j)\n",
        "        data.append(v)\n",
        "    return csr_matrix((data, (rows, cols)), shape=(n_rows, n_cols))\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hj9JPI1-o9YG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "157bdc7b-701a-4b04-df5f-927a5bf540a4"
      },
      "source": [
        "with open('./lalaland_komoran.txt', encoding='utf-8') as f:\n",
        "    sents = [sent.strip() for sent in f]\n",
        "\n",
        "with open('./lalaland.txt', encoding='utf-8') as f:\n",
        "    texts = [sent.strip() for sent in f]\n",
        "\n",
        "print(len(sents), len(texts))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15595 15595\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYjjNUHtpMKG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "a14a3113-0617-426e-e46d-19271a557311"
      },
      "source": [
        "def komoran_tokenize(sent):\n",
        "    words = sent.split()\n",
        "    words = [w for w in words if ('/NN' in w or '/XR' in w or '/VA' in w or '/VV' in w)]\n",
        "    return words\n",
        "\n",
        "keyword_extractor = KeywordSummarizer(\n",
        "    tokenize = komoran_tokenize,\n",
        "    window = -1,\n",
        "    verbose = False\n",
        ")\n",
        "keywords = keyword_extractor.summarize(sents, topk=30)\n",
        "for word, rank in keywords:\n",
        "    print('{} ({:.3})'.format(word, rank))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "영화/NNG (1.73e+02)\n",
            "보/VV (1.29e+02)\n",
            "좋/VA (65.5)\n",
            "하/VV (52.0)\n",
            "것/NNB (47.4)\n",
            "같/VA (45.4)\n",
            "영화/NNP (43.8)\n",
            "음악/NNG (43.6)\n",
            "꿈/NNG (41.4)\n",
            "있/VV (40.8)\n",
            "없/VA (35.9)\n",
            "마지막/NNG (31.9)\n",
            "수/NNB (30.1)\n",
            "사랑/NNG (28.3)\n",
            "아름답/VA (26.5)\n",
            "현실/NNG (24.8)\n",
            "되/VV (23.9)\n",
            "노래/NNG (23.4)\n",
            "생각/NNG (23.2)\n",
            "스토리/NNP (21.4)\n",
            "번/NNB (20.3)\n",
            "거/NNB (19.7)\n",
            "최고/NNG (19.2)\n",
            "때/NNG (19.1)\n",
            "사람/NNG (19.0)\n",
            "여운/NNP (17.5)\n",
            "뮤지컬/NNP (16.9)\n",
            "나오/VV (16.5)\n",
            "듯/NNB (16.1)\n",
            "영상미/NNG (16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_m9zlbAHpNQd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "69958977-f243-46b1-c13e-5685b482d78c"
      },
      "source": [
        "summarizer = KeysentenceSummarizer(\n",
        "    tokenize = komoran_tokenize,\n",
        "    min_sim = 0.5,\n",
        "    verbose = True\n",
        ")\n",
        "keysents = summarizer.summarize(sents)\n",
        "for idx, rank, komoran_sent in keysents:\n",
        "    print('#{} ({:.3}) : {}'.format(idx, rank, texts[idx]), end='\\n\\n')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "calculating textrank sentence similarity was done with 15595 sents\n",
            "trained TextRank. n sentences = 15595\n",
            "#5861 (6.12) : 사랑에 대해 다시 한 번 생각해 볼 수 있게 하는 영화인 것 같습니다 장면 처리도 좋았어요 여운이 많이 남는 영화입니다 꼭 보세요\n",
            "\n",
            "#5947 (5.8) : 아 진짜 평점 처음 써본다 진짜 후회 안할 영화 나중에 다시 봐도 좋을것 같다 오프닝에서 신나는 노래부터 마지막의 상상 하는 씬까지 너무 좋음 결말이 여운이 있다고 해야하나 슬프다고 해야하나 꿈얘기 할때 현실성 있어서 눈물날뻔 결말이짱\n",
            "\n",
            "#5076 (5.69) : 옛날 영화같은 느낌의 기법 사람을 행복하게 만드는 음악 약간의 촌스러움이 마음을 간지를 수 있는 지극히 현실적인 사랑과 꿈이야기를 로맨틱하게 풀어낸 영화로 연말 영화로 보기 좋은 것 같아요\n",
            "\n",
            "#6665 (5.41) : 인생영화다 노래도 너무 좋고 배우 소품 배경 장면들 하나하나 맘에 안 드는게 없다 ㅠㅠ 특히 마지막 셉oo에서의 내용은 진짜 잊을 수가 없을 거같다 보고나면 먹먹하고 안타까운 느낌이 드는데 그래도 황홀하고 아름다운 영화다\n",
            "\n",
            "#9271 (5.28) : 연출 음악 영상미 엔딩은 정말 좋았다 마지막에 남녀주인공이 나눈 눈빛이 아직도 잊혀지지않을만큼 여운이 남는 영화였다 초중반 약간 지루하긴했었다 배우들 춤연습을 많이한게보였음 꿈 성공 과 사랑을 다 가질수 없다는것을 현실적으로보여준 영화가아니었나싶다\n",
            "\n",
            "#13909 (5.12) : 인생 최고의 영화 또보고싶다 영상미 음악 스토리 다 좋아요\n",
            "\n",
            "#5922 (4.99) : 정말 영상미랑 음악은 최고였다 그리고 신선했다 음악이 너무 멋있어서 연기를 봐야 할지 노래를 들어야 할지 모를 정도로 그리고 보고 나서 생각 좀 많아진 영화 정말 이 연말에 보기 좋은 영화 인 것 같다\n",
            "\n",
            "#11408 (4.88) : 진짜 그냥 좋았던 영화 두번봐도 재밌을영화\n",
            "\n",
            "#12362 (4.88) : 보고 난 후 그냥 인생영화 다 라는 말밖에 안나오는 영화다 너무나도 아름다운 색감과 영상미 우리가 꿈꾸는 판타지 로맨스를 그대로 옮겨놓은 듯 하다 영화관을 나와도 귀에 맴도는 음악을 들으며 하늘에 있는 별들을 바라보며 집으로 올수밖에 없었다\n",
            "\n",
            "#187 (4.88) : 저가 본 영화중에서 두번째로 최고인 영화였던것같습니다 노래도너무좋았고 정말 한 장면도 놓칠수없었습니다 재밌었고 앞으로도 이런 비슷한 영화들이나와도 괜찮을것같다 싶었던것같습니다\n",
            "\n",
            "#2082 (4.84) : 뮤지컬영화 처음봐서 처음엔 이게 뭐지 싶었는데 노래가 영화를 얼마나 풍성하게 할 수 있는지 몸소 느꼈다 꿈을 향한 두 남녀의 여정과 사랑이 너무 아름다웠다 참 여운이 남는 영화\n",
            "\n",
            "#14711 (4.84) : 제 인생 최고의 뮤지컬 영화라고 자신있게 말할수 있어요 ㅜ ㅜ 또 보러 갈 예정 음악도 전부 좋고 엔딩크래딧 다 올라갈때까지 자리에서 여윤 때문에 못일어났네요 정말 아름다운 영화였어요 여운이 기네요\n",
            "\n",
            "#980 (4.77) : 좋은 영화다여운이 남고 마지막 장면은 눈물을 참을 수가 없었다 슬픈 눈물이 아닌 아름다운 것을 봤을 때 흘리는 눈물이런 기분이구나\n",
            "\n",
            "#5220 (4.72) : 음악 미술 연기 등 모든 것이 좋았지만 마지막 결말이 너무 현실에 뒤떨어진 꿈만 같다 꿈을 이야기하는 영화지만 과정과 결과에 있어 예술가들의 현실을 너무 반영하지 못한 것이 아닌가하는 생각이든다 그래서 보고 난 뒤 나는 꿈을 꿔야하는데 허탈했다\n",
            "\n",
            "#3099 (4.72) : 세 번째 관람 음악도 좋고 색감도 너무 이뻐요꿈에 대해 생각해 보게 하는 영화\n",
            "\n",
            "#8087 (4.7) : 조금 이해못했었는데 해석을 보니까 다 이해가 되더라고요 꿈과 사랑에 대해서 잘 표현한것 같았어요 이해하니까 여운이 많이 남았어요 노래도 되게 좋았고요 연기도 최고 2016년 말을 장식하는 최고의 영화에요 굿굿\n",
            "\n",
            "#7461 (4.69) : 인생영화가 아니라 인생 최악의 영화 영상미는 좋았지만 싸우는 장면 빼곤 지루하기 짝이 없음 결말도 너무나 어이없었음 평점 후기 보고 기대했는데 어떻게 내 생각과 이렇게 반대일 수 있는지 시간 돈이 아까웠다\n",
            "\n",
            "#10282 (4.63) : 여운 많이 남는 최고의 영화 영상미도 너무 아름답고 보는 내내 가슴이 벅차올랐다 재즈 음악이 많이 들리던 것도 너무 좋다 또 보러갈 예정\n",
            "\n",
            "#583 (4.61) : 눈이 호강하네요 음악도 좋구요 그러나 위플래쉬때와 같이 보고 나면 마음속에 남는것이 하나도 없습니다 음악영화이나 너무 보여주기식으로만 영화를 만드는것이 이 감독의 특징인 것 같습니다\n",
            "\n",
            "#9175 (4.6) : 너무 좋은 영화에요 꼭 보세요\n",
            "\n",
            "#11283 (4.6) : 아주 좋은 영화를 봤습니다\n",
            "\n",
            "#1929 (4.6) : 두고두고 계속 볼 영화 너무나 좋았습니다\n",
            "\n",
            "#2165 (4.6) : 너무 좋았어요 다시 보고싶은 영화에요\n",
            "\n",
            "#6068 (4.6) : 그냥 너랑 봐서 좋았던 영화\n",
            "\n",
            "#11603 (4.6) : 좋은 영화였습니다 또보고싶어요\n",
            "\n",
            "#1242 (4.6) : 정말 좋은 영화였어요 꼭 보세요\n",
            "\n",
            "#3869 (4.59) : 정말 너무 영화 잘 봤습니다 근래들어 본 영화중에 제일 감동 음악도 너무 좋고 배우들도 너무 멋있습니다 마지막 둘의 눈빛 잊을 수가 없네요\n",
            "\n",
            "#7059 (4.57) : 사실 두번째 보는 영화입니다 영상 편집과 음악이 너무 좋아요 어떻게 보면 너무나 현실적일 수 있는 결말이 슬프기하지만 아름답습니다\n",
            "\n",
            "#4033 (4.57) : 다른 분들에 비해 저는 조금 지루하긴 했는데 영상미는 끝내주는 것 같아요 음악도 영화 볼 때보다 보고나서 티비나 거리에서 들려오면 영화생각 나면서 더 좋은 듯\n",
            "\n",
            "#4357 (4.56) : 세번 봤어요 이동진 평론가 평 보고 다시 보니까 처음 봤을 때 말로 표현할 수 없던 감정들이 정리가 되는 느낌이어서 더 좋았어요 사랑영화이기도 하면서 성장영화이기도 한 라라랜드 인생영화에요\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvJPiOLpvK-G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "e2b3a8fa-0542-49cc-f5b2-b81703d39830"
      },
      "source": [
        "import pandas as pd\n",
        "f=pd.read_csv('/content/final_train_data_3.txt', delimiter = '\\t')\n",
        "f.head()"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review_id</th>\n",
              "      <th>review</th>\n",
              "      <th>rating</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2731670</td>\n",
              "      <td>특별하진 않지만 사랑스러운 작품</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4709715</td>\n",
              "      <td>실수를 연발하는그녀가 이제 더이상 사랑스럽다기보다 ㅇ나ㅆ</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>111222</td>\n",
              "      <td>무드만 있고 현상이없다. 스타일만있고 공허하다.</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>132464</td>\n",
              "      <td>충격</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>64730</td>\n",
              "      <td>가볍고 유쾌한 영화.</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   review_id                           review  rating  label\n",
              "0    2731670                특별하진 않지만 사랑스러운 작품       7      2\n",
              "1    4709715  실수를 연발하는그녀가 이제 더이상 사랑스럽다기보다 ㅇ나ㅆ       2      1\n",
              "2     111222       무드만 있고 현상이없다. 스타일만있고 공허하다.       5      2\n",
              "3     132464                               충격       5      2\n",
              "4      64730                      가볍고 유쾌한 영화.       8      2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3hlWj0OtnY1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7462d561-b480-4f0d-8278-7645c7266d5e"
      },
      "source": [
        "texts = f['review']\n",
        "texts[1]"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'실수를 연발하는그녀가 이제 더이상 사랑스럽다기보다 ㅇ나ㅆ'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCE_5I4mxltG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "outputId": "6c4992bd-2e12-4600-a01f-4195d0996669"
      },
      "source": [
        "from konlpy.tag import Komoran\n",
        "kkma = Kkma()\n",
        "print(kkma.pos(\"안녕하세요\"))\n",
        "for i in range(len(texts)):\n",
        "    for j, w in enumerate(kkma.pos(texts[i])):\n",
        "        texts[i] = w[j][0] + '/' + w[j][1] + ' '"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('안녕', 'NNG'), ('하', 'XSV'), ('세요', 'EFN')]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-82-bd3e24f68af2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkkma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mtexts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrcJIpaN2Z4A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b834bf03-9a19-403b-e410-779e266ec1a9"
      },
      "source": [
        "w[0]"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'지'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_6hfP7Tqh50",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "outputId": "15281bfb-54b8-4d07-9f23-ddbf233b87cc"
      },
      "source": [
        "with open('./lalaland_komoran.txt', encoding='utf-8') as f:\n",
        "    sents = [sent.strip() for sent in f]\n",
        "\n",
        "with open('./lalaland.txt', encoding='utf-8') as f:\n",
        "    texts = [sent.strip() for sent in f]\n",
        "\n",
        "def komoran_tokenize(sent):\n",
        "    words = sent.split()\n",
        "    words = [w for w in words if ('/NN' in w or '/XR' in w or '/VA' in w or '/VV' in w)]\n",
        "    return words\n",
        "\n",
        "keyword_extractor = KeywordSummarizer(\n",
        "    tokenize = komoran_tokenize,\n",
        "    window = -1,\n",
        "    verbose = False\n",
        ")\n",
        "keywords = keyword_extractor.summarize(sents, topk=30)\n",
        "\n",
        "summarizer = KeysentenceSummarizer(\n",
        "    tokenize = komoran_tokenize,\n",
        "    min_sim = 0.5,\n",
        "    verbose = True\n",
        ")\n",
        "keysents = summarizer.summarize(sents)\n",
        "for idx, rank, komoran_sent in keysents:\n",
        "    print('#{} ({:.3}) : {}'.format(idx, rank, texts[idx]), end='\\n\\n')"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mndim\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m   3069\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3070\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3071\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'ndim'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-b44857d457e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m )\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mkeywords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeyword_extractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m summarizer = KeysentenceSummarizer(\n",
            "\u001b[0;32m<ipython-input-3-896cc2f9d322>\u001b[0m in \u001b[0;36msummarize\u001b[0;34m(self, sents, topk)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msummarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_textrank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-896cc2f9d322>\u001b[0m in \u001b[0;36mtrain_textrank\u001b[0;34m(self, sents, bias)\u001b[0m\n\u001b[1;32m     23\u001b[0m         g, self.idx_to_vocab = word_graph(sents,\n\u001b[1;32m     24\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             self.min_cooccurrence, self.vocab_to_idx, self.verbose)\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpagerank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-98cec4852ca2>\u001b[0m in \u001b[0;36mword_graph\u001b[0;34m(sents, tokenize, min_count, window, min_cooccurrence, vocab_to_idx, verbose)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcooccurrence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_cooccurrence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_to_vocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-98cec4852ca2>\u001b[0m in \u001b[0;36mcooccurrence\u001b[0;34m(tokens, vocab_to_idx, window, min_cooccurrence, verbose)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\rword cooccurrence counting from {} sents was done'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdict_to_mat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_vocabs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_vocabs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdict_to_mat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-98cec4852ca2>\u001b[0m in \u001b[0;36mdict_to_mat\u001b[0;34m(d, n_rows, n_cols)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mcols\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcsr_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0misshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m                 \u001b[0;31m# It's a tuple of matrix dimensions (M, N)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0;31m# create empty matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/scipy/sparse/sputils.py\u001b[0m in \u001b[0;36misshape\u001b[0;34m(x, nonneg)\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0misintlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misintlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnonneg\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mN\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/scipy/sparse/sputils.py\u001b[0m in \u001b[0;36misintlike\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;31m# Fast-path check to eliminate non-scalar values. operator.index would\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0;31m# catch this case too, but the exception catching is slow.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mndim\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mndim\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m   3070\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3071\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3072\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tp21A5Oz2M-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}